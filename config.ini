[DEFAULT]
# Prints local model training.
verbose: false
data_augmentation: true

[Federated]
num_rounds: 20
num_participants: 100
num_selected: 10
# Controls how many local models to train in parallel
parallel_streams: 1
local_epochs: 2
local_lr: 0.1
# Also referred to as eta
global_lr: 1
batch_size: 64
save_interval: 100
# Dirichlet concentration paremeter for non-i.i.d sampling
alpha: 0.9
# Path to partioned idxs
partition_path: ./partitions.json
# Load model and train ontop rather than from scratch
load_from_checkpoint : ./global_model_pretrain_best.pt

[Pretraining]
pretrain: false
lr: 0.001
epochs: 200
batch_size: 128
save_interval: 10

[Poison]
local_epochs: 6
local_lr: 0.05
lr_step_size: 2
lr_gamma: 0.1
batch_size: 64
poison_per_batch: 20
target_idx: 2
# Number of times to sample the test set for evaluation.
num_eval: 1000
poison_round: 5
train_idxs: 2180
            2771
            3233
            4932
            6241
            6813
            9476
            11395
            11744
            14209
            14238
            19793
            21529
            31311
            40518
            40633
            42119
            42663
            49392

test_idxs:  6869
            18716
            20781