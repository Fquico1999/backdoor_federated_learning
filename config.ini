[DEFAULT]
# Prints local model training.
verbose: false
data_augmentation: true

[Federated]
num_rounds: 10
num_participants: 100
num_selected: 10
# Controls how many local models to train in parallel
parallel_streams: 2
local_epochs: 2
local_lr: 0.1
# Also referred to as eta
global_lr: 1
batch_size: 32
save_interval: 10
# Dirichlet concentration paremeter for non-i.i.d sampling
alpha: 0.9
# Path to partioned idxs
partition_path: ./partitions.json
# Load model and train ontop rather than from scratch
load_from_checkpoint :

[Pretraining]
pretrain: true
lr: 0.001
epochs: 100
batch_size: 128
save_interval: 10

[Poison]
target_idx: 1
train_idxs: 2180
            2771
            3233
            4932
            6241
            6813
            9476
            11395
            11744
            14209
            14238
            19793
            21529
            31311
            40518
            40633
            42119
            42663
            49392

test_idxs:  6869
            18716
            20781