[DEFAULT]
# Prints local model training.
verbose: true
data_augmentation: true

[Federated]
num_rounds: 10000
num_participants: 100
num_selected: 10
# Controls how many local models to train in parallel
parallel_streams: 10
local_epochs: 2
local_lr: 0.01
# Also referred to as eta
global_lr: 1
batch_size: 64
save_interval: 100
# Dirichlet concentration paremeter for non-i.i.d sampling
alpha: 0.9
# Path to partioned idxs
partition_path: ./partitions.json
# Load model and train ontop rather than from scratch
load_from_checkpoint : ./models/global_model_round_2000.pt

[Pretraining]
pretrain: false
lr: 0.001
epochs: 100
batch_size: 128
save_interval: 10

[Poison]
local_epochs: 12
local_lr: 0.05
lr_step_size: 4
lr_gamma: 0.1
batch_size: 64
poison_per_batch: 20
target_idx: 2
# Number of times to sample the test set for evaluation.
num_eval: 1000
poison_round: 12000
train_idxs: 568
            3934
            30560
            30696
            33105
            33615
            33907
            36848
            40713

test_idxs:  41706
            330
            12336


